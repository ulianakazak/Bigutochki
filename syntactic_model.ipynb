{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем csv с векторами предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_link(response):\n",
    "    spamreader = csv.reader(response)\n",
    "    vectors = []\n",
    "    i = 0\n",
    "    for row in spamreader:\n",
    "        if i == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        row = [float(el) for el in row[1:]]\n",
    "        vectors.append(row)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем расстояния между векторами предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distances_link(file1, file2):\n",
    "    vectors_1 = read_csv_link(file1)\n",
    "    vectors_2 = read_csv_link(file2)\n",
    "    vectors_1 = np.array([np.array(vectors_1[i]) for i in range(len(vectors_1))])\n",
    "    vectors_2 = np.array([np.array(vectors_2[i]) for i in range(len(vectors_2))])\n",
    "    dist_1 = vectors_1 - vectors_2\n",
    "    \n",
    "    euclidean = paired_distances(vectors_1, vectors_2, metric='euclidean')\n",
    "    manhattan= paired_distances(vectors_1, vectors_2, metric='manhattan')\n",
    "    cosine = paired_distances(vectors_1, vectors_2, metric='cosine')\n",
    "\n",
    "    distances =  np.array([euclidean, manhattan, cosine]).T\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_val_1_1 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/val_1_1.csv')\n",
    "response_val_2_1 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/val_2_1.csv')\n",
    "response_val_1_2 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/val_1_2.csv')\n",
    "response_val_2_2 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/val_2_2.csv')\n",
    "response_test_1_1 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/test_1_1.csv')\n",
    "response_test_2_1 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/test_2_1.csv')\n",
    "response_test_1_2 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/test_1_2.csv')\n",
    "response_test_2_2 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/test_2_2.csv')\n",
    "\n",
    "X_val_1 = count_distances_link(io.TextIOWrapper(response_val_1_1), io.TextIOWrapper(response_val_2_1))\n",
    "X_val_2 = count_distances_link(io.TextIOWrapper(response_val_1_2), io.TextIOWrapper(response_val_2_2))\n",
    "X_val = np.vstack((X_val_1, X_val_2))\n",
    "\n",
    "X_test_1 = count_distances_link(io.TextIOWrapper(response_test_1_1), io.TextIOWrapper(response_test_2_1))\n",
    "X_test_2 = count_distances_link(io.TextIOWrapper(response_test_1_2), io.TextIOWrapper(response_test_2_2))\n",
    "X_test = np.vstack((X_test_1, X_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/data_syntax/train_1_0.csv')\n",
    "response_2 = urllib.request.urlopen('https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/data_syntax/train_2_0.csv')\n",
    "X_train = count_distances_link(io.TextIOWrapper(response_1), io.TextIOWrapper(response_2))\n",
    "for i in range(1, 69):\n",
    "    response_1 = urllib.request.urlopen(f'https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/data_syntax/train_1_{i}.csv')\n",
    "    response_2 = urllib.request.urlopen(f'https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/data_syntax/train_2_{i}.csv')\n",
    "    X = count_distances_link(io.TextIOWrapper(response_1), io.TextIOWrapper(response_2))\n",
    "    X_train = np.vstack((X_train, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import fbeta_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('snli_1.0_train.jsonl', lines = True)\n",
    "test = pd.read_json('snli_1.0_test.jsonl', lines = True)\n",
    "val = pd.read_json('snli_1.0_dev.jsonl', lines = True)\n",
    "full = {'train': train, 'val':val, 'test': test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unsure_labels(data):\n",
    "    data = data.drop(index = data[data.gold_label == '-'].index)\n",
    "    return data\n",
    "\n",
    "\n",
    "for frame in full:\n",
    "    full[frame] = delete_unsure_labels(full[frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unsure_labels_vec(X, data):\n",
    "    X = X[data.index]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full['train']\n",
    "val = full['val']\n",
    "test = full['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = delete_unsure_labels_vec(X_train, train)\n",
    "X_val = delete_unsure_labels_vec(X_val, val)\n",
    "X_test = delete_unsure_labels_vec(X_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train['gold_label'])\n",
    "y_val = np.array(val['gold_label'])\n",
    "y_test = np.array(test['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.21737106 0.57207067 0.20949664]\n",
      "val: [0.92043663 0.02754214 0.01701473]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver = 'saga')\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_preds = clf.predict(X_train)\n",
    "f_train = fbeta_score(y_train, y_train_preds, beta=7, average=None,)\n",
    "y_val_preds = clf.predict(X_val)\n",
    "f_val = fbeta_score(y_val, y_val_preds, beta=7, average=None,)\n",
    "print(f'train: {f_train}')\n",
    "print(f'val: {f_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.28775785, 0.28881738, 0.28754821])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver = 'saga')\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=3, average='weighted')\n",
    "cross_val_score(clf, X_test, y_test, cv=3, scoring= ftwo_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.09069626 0.78048526 0.16737006]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver = 'saga')\n",
    "clf.fit(X_test, y_test)\n",
    "y_train_preds = clf.predict(X_test)\n",
    "f_train = fbeta_score(y_test, y_train_preds, beta=7, average=None,)\n",
    "y_val_preds = clf.predict(X_val)\n",
    "f_val = fbeta_score(y_val, y_val_preds, beta=7, average=None,)\n",
    "print(f'train: {f_train}')\n",
    "print(f'val: {f_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>contradiction</th>\n",
       "      <td>0.332766</td>\n",
       "      <td>0.954851</td>\n",
       "      <td>0.920437</td>\n",
       "      <td>3278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.016692</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>3235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entailment</th>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>3329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted*</th>\n",
       "      <td>0.330236</td>\n",
       "      <td>0.488357</td>\n",
       "      <td>0.471358</td>\n",
       "      <td>3280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision    recall    fscore  support\n",
       "contradiction   0.332766  0.954851  0.920437   3278.0\n",
       "neutral         0.315789  0.016692  0.017015   3235.0\n",
       "entailment      0.339623  0.027035  0.027542   3329.0\n",
       "Weighted*       0.330236  0.488357  0.471358   3280.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prfs = precision_recall_fscore_support(y_val, y_val_preds, beta=7, average=None, labels=['contradiction', 'neutral', 'entailment'])\n",
    "matrix_report = pd.DataFrame(prfs, columns = ['contradiction', 'neutral', 'entailment'], index = ['precision', 'recall', 'fscore', 'support'])\n",
    "matrix_report['Weighted*'] = 0.5 * matrix_report['contradiction'] +  0.25 * matrix_report['neutral'] +  0.25 * matrix_report['entailment']\n",
    "matrix_report.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
