{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import fbeta_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('snli_1.0_train.jsonl', lines = True)\n",
    "test = pd.read_json('snli_1.0_test.jsonl', lines = True)\n",
    "val = pd.read_json('snli_1.0_dev.jsonl', lines = True)\n",
    "full = {'train': train, 'val':val, 'test': test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unsure_labels(data):\n",
    "    data = data.drop(index = data[data.gold_label == '-'].index)\n",
    "    return data\n",
    "\n",
    "\n",
    "for frame in full:\n",
    "    full[frame] = delete_unsure_labels(full[frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full['train']\n",
    "val = full['val']\n",
    "test = full['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем в данные столбцы с лемматизированными предложениями и со средними векторами каждого предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vec(words, model, num_features):\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg_vec(data, sent, model):\n",
    "    data[f'{sent}_lemmas'] = data[sent].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    data[f'{sent}_avgvec'] = data[f'{sent}_lemmas'].apply(lambda x: avg_vec(x, model, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train, val, test]:\n",
    "    for sent in ['sentence1', 'sentence2']:\n",
    "        add_avg_vec(data, sent, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем схожесть двух предложений (одно число)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similarity(data):\n",
    "    data['similarity'] = data[['sentence1_avgvec', 'sentence2_avgvec']].apply(lambda x: 1 - spatial.distance.cosine(x['sentence1_avgvec'], x['sentence2_avgvec']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычитаем один вектор из другого (получаем вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distance(data):\n",
    "    data['distance'] = data[['sentence1_avgvec', 'sentence2_avgvec']].apply(lambda x: x['sentence1_avgvec'] - x['sentence2_avgvec'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно добавить еще какие-нибудь метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train, val, test]:\n",
    "    count_distance(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "for data in [train, val, test]:\n",
    "    count_similarity(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будем обучать на расстоянии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train['distance'].tolist())\n",
    "y_train = np.array(train['gold_label'])\n",
    "X_val = np.array(val['distance'].tolist())\n",
    "y_val = np.array(val['gold_label'])\n",
    "X_test = np.array(test['distance'].tolist())\n",
    "y_test = np.array(test['gold_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будем обучать на схожести"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90185082]\n",
      " [0.84060919]\n",
      " [0.91352427]\n",
      " ...\n",
      " [0.95085871]\n",
      " [0.88993967]\n",
      " [0.98977244]]\n",
      "['neutral' 'contradiction' 'entailment' ... 'neutral' 'contradiction'\n",
      " 'entailment']\n"
     ]
    }
   ],
   "source": [
    "X_train = train['similarity'].values.reshape(-1,1)\n",
    "y_train = np.array(train['gold_label'])\n",
    "X_val = val['similarity'].values.reshape(-1,1)\n",
    "y_val = np.array(val['gold_label'])\n",
    "X_test = test['similarity'].values.reshape(-1,1)\n",
    "y_test = np.array(test['gold_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тренировочных данных появились откуда-то пропущенные значения, я не поняла, откуда, поэтому просто удалила их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84060919],\n",
       "       [0.91352427],\n",
       "       [0.83683312],\n",
       "       ...,\n",
       "       [0.95085871],\n",
       "       [0.88993967],\n",
       "       [0.98977244]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeces = np.argwhere(np.isnan(X_train))\n",
    "X_train = np.delete(X_train, indeces).reshape(-1,1)\n",
    "y_train = np.delete(y_train, indeces)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.53739805 0.62706275 0.56522514]\n",
      "val: [0.52235116 0.62097606 0.57441872]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=100, solver = 'saga')\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_preds = clf.predict(X_train)\n",
    "f_train = fbeta_score(y_train, y_train_preds, beta=7, average=None,)\n",
    "y_val_preds = clf.predict(X_val)\n",
    "f_val = fbeta_score(y_val, y_val_preds, beta=7, average=None,)\n",
    "print(f'train: {f_train}')\n",
    "print(f'val: {f_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39209549, 0.39414138, 0.39151485])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver = 'saga')\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=3, average='weighted')\n",
    "cross_val_score(clf, X_train, y_train, cv=3, scoring= ftwo_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>contradiction</th>\n",
       "      <td>0.564128</td>\n",
       "      <td>0.521965</td>\n",
       "      <td>0.522746</td>\n",
       "      <td>3278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.582080</td>\n",
       "      <td>0.574343</td>\n",
       "      <td>0.574496</td>\n",
       "      <td>3235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entailment</th>\n",
       "      <td>0.572021</td>\n",
       "      <td>0.621508</td>\n",
       "      <td>0.620434</td>\n",
       "      <td>3329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted*</th>\n",
       "      <td>0.570589</td>\n",
       "      <td>0.559945</td>\n",
       "      <td>0.560106</td>\n",
       "      <td>3280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision    recall    fscore  support\n",
       "contradiction   0.564128  0.521965  0.522746   3278.0\n",
       "neutral         0.582080  0.574343  0.574496   3235.0\n",
       "entailment      0.572021  0.621508  0.620434   3329.0\n",
       "Weighted*       0.570589  0.559945  0.560106   3280.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prfs = precision_recall_fscore_support(y_val, y_val_preds, beta=7, average=None, labels=['contradiction', 'neutral', 'entailment'])\n",
    "matrix_report = pd.DataFrame(prfs, columns = ['contradiction', 'neutral', 'entailment'], index = ['precision', 'recall', 'fscore', 'support'])\n",
    "matrix_report['Weighted*'] = 0.5 * matrix_report['contradiction'] +  0.25 * matrix_report['neutral'] +  0.25 * matrix_report['entailment']\n",
    "matrix_report.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
