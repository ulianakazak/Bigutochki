{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import fbeta_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('snli_1.0_train.jsonl', lines = True)\n",
    "test = pd.read_json('snli_1.0_test.jsonl', lines = True)\n",
    "val = pd.read_json('snli_1.0_dev.jsonl', lines = True)\n",
    "full = {'train': train, 'val':val, 'test': test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unsure_labels(data):\n",
    "    data = data.drop(index = data[data.gold_label == '-'].index)\n",
    "    return data\n",
    "\n",
    "\n",
    "for frame in full:\n",
    "    full[frame] = delete_unsure_labels(full[frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = full['train']\n",
    "val = full['val']\n",
    "test = full['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем в данные столбцы с лемматизированными предложениями и со средними векторами каждого предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vec(words, model, num_features):\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg_vec(data, sent, model):\n",
    "    data[f'{sent}_lemmas'] = data[sent].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    data[f'{sent}_avgvec'] = data[f'{sent}_lemmas'].apply(lambda x: avg_vec(x, model, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train, val, test]:\n",
    "    for sent in ['sentence1', 'sentence2']:\n",
    "        add_avg_vec(data, sent, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distances(data):\n",
    "    # преобразует предложения и считает расстояния(евклидово, матхэт., косинусное) между преобразованными предложениями\n",
    "    vec_1 = np.array(data['sentence1_avgvec'].tolist())\n",
    "    vec_2 = np.array(data['sentence2_avgvec'].tolist())\n",
    "    euclidean = paired_distances(vec_1, vec_2, metric='euclidean')\n",
    "    manhattan= paired_distances(vec_1, vec_2, metric='manhattan')\n",
    "    cosine = paired_distances(vec_1, vec_2, metric='cosine')\n",
    "\n",
    "    distances =  np.array([euclidean, manhattan, cosine]).T\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = count_distances(train)\n",
    "y_train = np.array(train['gold_label'])\n",
    "X_val = count_distances(val)\n",
    "y_val = np.array(val['gold_label'])\n",
    "X_test = count_distances(test)\n",
    "y_test = np.array(test['gold_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.5664823  0.64554178 0.13013437]\n",
      "val: [0.57963358 0.64230574 0.1359036 ]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=100, solver = 'saga')\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_preds = clf.predict(X_train)\n",
    "f_train = fbeta_score(y_train, y_train_preds, beta=7, average=None,)\n",
    "y_val_preds = clf.predict(X_val)\n",
    "f_val = fbeta_score(y_val, y_val_preds, beta=7, average=None,)\n",
    "print(f'train: {f_train}')\n",
    "print(f'val: {f_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4381281 , 0.44300944, 0.44016212])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=500, solver = 'saga')\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=3, average='weighted')\n",
    "cross_val_score(clf, X_train, y_train, cv=3, scoring= ftwo_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>contradiction</th>\n",
       "      <td>0.464008</td>\n",
       "      <td>0.582062</td>\n",
       "      <td>0.579115</td>\n",
       "      <td>3278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.134158</td>\n",
       "      <td>0.135886</td>\n",
       "      <td>3235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entailment</th>\n",
       "      <td>0.457113</td>\n",
       "      <td>0.646741</td>\n",
       "      <td>0.641419</td>\n",
       "      <td>3329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted*</th>\n",
       "      <td>0.438387</td>\n",
       "      <td>0.486256</td>\n",
       "      <td>0.483884</td>\n",
       "      <td>3280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision    recall    fscore  support\n",
       "contradiction   0.464008  0.582062  0.579115   3278.0\n",
       "neutral         0.368421  0.134158  0.135886   3235.0\n",
       "entailment      0.457113  0.646741  0.641419   3329.0\n",
       "Weighted*       0.438387  0.486256  0.483884   3280.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prfs = precision_recall_fscore_support(y_val, y_val_preds, beta=7, average=None, labels=['contradiction', 'neutral', 'entailment'])\n",
    "matrix_report = pd.DataFrame(prfs, columns = ['contradiction', 'neutral', 'entailment'], index = ['precision', 'recall', 'fscore', 'support'])\n",
    "matrix_report['Weighted*'] = 0.5 * matrix_report['contradiction'] +  0.25 * matrix_report['neutral'] +  0.25 * matrix_report['entailment']\n",
    "matrix_report.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
