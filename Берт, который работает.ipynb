{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"BERT.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulianakazak/Bigutochki/blob/main/%D0%91%D0%B5%D1%80%D1%82%2C%20%D0%BA%D0%BE%D1%82%D0%BE%D1%80%D1%8B%D0%B9%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D0%B5%D1%82.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFxy9VfywWj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "471930a0-cbd7-4b77-c591-25ad7e1bfc56"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# https://github.com/google-research/bert\n",
        "# https://github.com/CyberZHG/keras-bert\n",
        "\n",
        "# папка, куда распаковать преодобученную нейросеть BERT\n",
        "folder = 'multi_cased_L-12_H-768_A-12'\n",
        "download_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'  # ссылка на скачивание модели\n",
        "\n",
        "print('Downloading model...')\n",
        "zip_path = '{}.zip'.format(folder)\n",
        "!test -d $folder || (wget $download_url && unzip $zip_path)\n",
        "\n",
        "# скачиваем из BERT репозитория файл tokenization.py\n",
        "!wget https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/tokenization.py\n",
        "\n",
        "# install Keras BERT\n",
        "!pip install keras-bert\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "import tokenization\n",
        "\n",
        "config_path = folder+'/bert_config.json'\n",
        "checkpoint_path = folder+'/bert_model.ckpt'\n",
        "vocab_path = folder+'/vocab.txt'\n",
        "\n",
        "# создаем объект для перевода строки с пробелами в токены\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
        "\n",
        "# загружаем модель\n",
        "print('Loading model...')\n",
        "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n",
        "#model.summary()          # информация о слоях нейросети - количество параметров и т.д.\n",
        "print('OK')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading model...\n",
            "--2021-03-13 18:50:13--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 2607:f8b0:400e:c08::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M   114MB/s    in 6.1s    \n",
            "\n",
            "2021-03-13 18:50:20 (103 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n",
            "Archive:  multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n",
            "--2021-03-13 18:50:26--  https://raw.githubusercontent.com/ulianakazak/Bigutochki/main/tokenization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12260 (12K) [text/plain]\n",
            "Saving to: ‘tokenization.py’\n",
            "\n",
            "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-03-13 18:50:27 (20.6 MB/s) - ‘tokenization.py’ saved [12260/12260]\n",
            "\n",
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-bert) (1.19.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp37-none-any.whl size=34144 sha256=402a8c1995cc7ac0368c979113577dd5cf642737d3243d2ff05472eb3d97b194\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp37-none-any.whl size=12942 sha256=2565a98efd73e6e1fffa4fae15c6f7f121fb385175202272934926fcad6e681c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp37-none-any.whl size=7554 sha256=5eb95bbde0d496f6ad665c0ad6c06802a7ed8fba5b5c47e859623f7bbfce1da8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp37-none-any.whl size=15611 sha256=e03ead92c7c9568470552e9d70e6eb39756df266c9935cda8ddb6bbe7223e7bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp37-none-any.whl size=5269 sha256=a36452869edbc0d72040b11c1ac98c62f2e7fcd5191d7b14e7f2b2dca667a115\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp37-none-any.whl size=5623 sha256=d31d6019fcbefc5c16e8a062243fac62e16592fbb95e2b8d0ebb5720ca275be4\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp37-none-any.whl size=4558 sha256=bd742b518dcdfd21d0e71e7758451c7fd3612026773e7085b6b127ba0e3f03c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp37-none-any.whl size=17278 sha256=0dbdbb56e56a9b86a1b4b41d5117a44eb866a0636869c6d4da65cadf75cc4ef5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n",
            "Loading model...\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqIbMZ8hj70_"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbb41HMujx5j"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import fbeta_score,make_scorer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIErUB4BjphA"
      },
      "source": [
        "#train = pd.read_json('snli_1.0_train.jsonl', lines = True)\n",
        "test = pd.read_json('snli_1.0_test.jsonl', lines = True)\n",
        "#val = pd.read_json('snli_1.0_dev.jsonl', lines = True)\n",
        "#full = {'train': train, 'val':val, 'test': test}\n",
        "#full = {'val':val, 'test': test}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "ka41hKLXlGTs",
        "outputId": "a9306224-0f34-4abd-a79c-9ac09b5e5aff"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotator_labels</th>\n",
              "      <th>captionID</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>pairID</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence1_binary_parse</th>\n",
              "      <th>sentence1_parse</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>sentence2_binary_parse</th>\n",
              "      <th>sentence2_parse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[neutral, contradiction, contradiction, neutra...</td>\n",
              "      <td>2677109430.jpg#1</td>\n",
              "      <td>neutral</td>\n",
              "      <td>2677109430.jpg#1r1n</td>\n",
              "      <td>This church choir sings to the masses as they ...</td>\n",
              "      <td>( ( This ( church choir ) ) ( ( ( sings ( to (...</td>\n",
              "      <td>(ROOT (S (NP (DT This) (NN church) (NN choir))...</td>\n",
              "      <td>The church has cracks in the ceiling.</td>\n",
              "      <td>( ( The church ) ( ( has ( cracks ( in ( the c...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN church)) (VP (VBZ ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[entailment, entailment, entailment, neutral, ...</td>\n",
              "      <td>2677109430.jpg#1</td>\n",
              "      <td>entailment</td>\n",
              "      <td>2677109430.jpg#1r1e</td>\n",
              "      <td>This church choir sings to the masses as they ...</td>\n",
              "      <td>( ( This ( church choir ) ) ( ( ( sings ( to (...</td>\n",
              "      <td>(ROOT (S (NP (DT This) (NN church) (NN choir))...</td>\n",
              "      <td>The church is filled with song.</td>\n",
              "      <td>( ( The church ) ( ( is ( filled ( with song )...</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN church)) (VP (VBZ is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[contradiction, contradiction, contradiction, ...</td>\n",
              "      <td>2677109430.jpg#1</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>2677109430.jpg#1r1c</td>\n",
              "      <td>This church choir sings to the masses as they ...</td>\n",
              "      <td>( ( This ( church choir ) ) ( ( ( sings ( to (...</td>\n",
              "      <td>(ROOT (S (NP (DT This) (NN church) (NN choir))...</td>\n",
              "      <td>A choir singing at a baseball game.</td>\n",
              "      <td>( ( ( A choir ) ( singing ( at ( a ( baseball ...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN choir)) (VP (VBG sing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral]</td>\n",
              "      <td>6160193920.jpg#4</td>\n",
              "      <td>neutral</td>\n",
              "      <td>6160193920.jpg#4r1n</td>\n",
              "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
              "      <td>( ( ( A woman ) ( with ( ( ( ( ( a ( green hea...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN woman)) (PP (IN with)...</td>\n",
              "      <td>The woman is young.</td>\n",
              "      <td>( ( The woman ) ( ( is young ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN woman)) (VP (VBZ is)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[entailment, entailment, contradiction, entail...</td>\n",
              "      <td>6160193920.jpg#4</td>\n",
              "      <td>entailment</td>\n",
              "      <td>6160193920.jpg#4r1e</td>\n",
              "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
              "      <td>( ( ( A woman ) ( with ( ( ( ( ( a ( green hea...</td>\n",
              "      <td>(ROOT (NP (NP (DT A) (NN woman)) (PP (IN with)...</td>\n",
              "      <td>The woman is very happy.</td>\n",
              "      <td>( ( The woman ) ( ( is ( very happy ) ) . ) )</td>\n",
              "      <td>(ROOT (S (NP (DT The) (NN woman)) (VP (VBZ is)...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    annotator_labels  ...                                    sentence2_parse\n",
              "0  [neutral, contradiction, contradiction, neutra...  ...  (ROOT (S (NP (DT The) (NN church)) (VP (VBZ ha...\n",
              "1  [entailment, entailment, entailment, neutral, ...  ...  (ROOT (S (NP (DT The) (NN church)) (VP (VBZ is...\n",
              "2  [contradiction, contradiction, contradiction, ...  ...  (ROOT (NP (NP (DT A) (NN choir)) (VP (VBG sing...\n",
              "3      [neutral, neutral, neutral, neutral, neutral]  ...  (ROOT (S (NP (DT The) (NN woman)) (VP (VBZ is)...\n",
              "4  [entailment, entailment, contradiction, entail...  ...  (ROOT (S (NP (DT The) (NN woman)) (VP (VBZ is)...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zikPBVg8wkjZ"
      },
      "source": [
        "\n",
        "# РЕЖИМ 1: предсказание слов, закрытых токеном MASK в фразе. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в [MASK] и купил [MASK]. [SEP]\n",
        "\n",
        "# входная фраза с закрытыми словами с помощью [MASK]\n",
        "sentence = 'Я пришел в [MASK] и купил [MASK].'  #@param {type:\"string\"}\n",
        "\n",
        "print(sentence)\n",
        "\n",
        "\n",
        "#-------------------------\n",
        "# преобразование в токены (tokenizer.tokenize() не обрабатывает [CLS], [MASK], поэтому добавим их вручную)\n",
        "sentence = sentence.replace(' [MASK] ','[MASK]'); sentence = sentence.replace('[MASK] ','[MASK]'); sentence = sentence.replace(' [MASK]','[MASK]')  # удаляем лишние пробелы\n",
        "sentence = sentence.split('[MASK]')             # разбиваем строку по маске\n",
        "tokens = ['[CLS]']                              # фраза всегда должна начинаться на [CLS]\n",
        "# обычные строки преобразуем в токены с помощью tokenizer.tokenize(), вставляя между ними [MASK]\n",
        "for i in range(len(sentence)):\n",
        "    if i == 0:\n",
        "        tokens = tokens + tokenizer.tokenize(sentence[i]) \n",
        "    else:\n",
        "        tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) \n",
        "tokens = tokens + ['[SEP]']                     # фраза всегда должна заканчиваться на [SEP] \n",
        "# в tokens теперь токены, которые гарантированно по словарю преобразуются в индексы\n",
        "#-------------------------\n",
        "#print(tokens)\n",
        "\n",
        "# преобразуем в массив индексов, который можно подавать на вход сети, причем число 103 в нем это [MASK]\n",
        "token_input = tokenizer.convert_tokens_to_ids(tokens)        \n",
        "#print(token_input)\n",
        "# удлиняем до 512 длины\n",
        "token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "\n",
        "# создаем маску, заменив все числа 103 на 1, а остальное 0\n",
        "mask_input = [0]*512\n",
        "for i in range(len(mask_input)):\n",
        "    if token_input[i] == 103:\n",
        "        mask_input[i] = 1\n",
        "#print(mask_input)\n",
        "\n",
        "# маска фраз (вторая фраза маскируется числом 1, а все остальное числом 0)\n",
        "seg_input = [0]*512\n",
        "\n",
        "\n",
        "# конвертируем в numpy в форму (1,) -> (1,512)\n",
        "token_input = np.asarray([token_input])\n",
        "mask_input = np.asarray([mask_input])\n",
        "seg_input = np.asarray([seg_input])\n",
        "\n",
        "\n",
        "# пропускаем через нейросеть...\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[0]       # в [0] полная фраза с заполненными предсказанными словами на месте [MASK]\n",
        "predicts = np.argmax(predicts, axis=-1)\n",
        "\n",
        "\n",
        "# форматируем результат в строку, разделенную пробелами\n",
        "predicts = predicts[0][:len(tokens)]    # длиной как исходная фраза (чтобы отсечь случайные выбросы среди нулей дальше)\n",
        "out = []\n",
        "# добавляем в out только слова в позиции [MASK], которые маскированы цифрой 1 в mask_input\n",
        "for i in range(len(mask_input[0])):\n",
        "    if mask_input[0][i] == 1:           # [0][i], т.к. требование было (1,512)\n",
        "        out.append(predicts[i]) \n",
        "\n",
        "out = tokenizer.convert_ids_to_tokens(out)      # индексы в токены\n",
        "out = ' '.join(out)                             # объединяем в одну строку с пробелами\n",
        "out = tokenization.printable_text(out)          # в читабельную версию\n",
        "out = out.replace(' ##','')                     # объединяем раздъединенные слова \"при ##шел\" -> \"пришел\"\n",
        "print('Result:', out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjXLJupvwoFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c204df91-8637-4f28-f3e2-2ee19cc4cc8b"
      },
      "source": [
        "\n",
        "# РЕЖИМ 2: проверка логичности двух фраз. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в магазин. [SEP] И купил молоко. [SEP]\n",
        "\n",
        "sentence_1 = 'Children smiling and waving at camera'      #@param {type:\"string\"}\n",
        "sentence_2 = 'They are smiling at their parents'          #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "print(sentence_1, '->', sentence_2)\n",
        "\n",
        "# строки в массивы токенов\n",
        "tokens_sen_1 = tokenizer.tokenize(sentence_1)\n",
        "tokens_sen_2 = tokenizer.tokenize(sentence_2)\n",
        "\n",
        "tokens = ['[CLS]'] + tokens_sen_1 + ['[SEP]'] + tokens_sen_2 + ['[SEP]']\n",
        "#print(tokens)\n",
        "\n",
        "# преобразуем строковые токены в числовые индексы:\n",
        "token_input = tokenizer.convert_tokens_to_ids(tokens)  \n",
        "# удлиняем до 512      \n",
        "token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "# маска в этом режиме все 0\n",
        "mask_input = [0] * 512\n",
        "\n",
        "# в маске предложений под второй фразой, включая конечный SEP, надо поставить 1, а все остальное заполнить 0\n",
        "seg_input = [0]*512\n",
        "len_1 = len(tokens_sen_1) + 2                   # длина первой фразы, +2 - включая начальный CLS и разделитель SEP\n",
        "for i in range(len(tokens_sen_2)+1):            # +1, т.к. включая последний SEP\n",
        "        seg_input[len_1 + i] = 1                # маскируем вторую фразу, включая последний SEP, единицами\n",
        "#print(seg_input)\n",
        "\n",
        "\n",
        "# конвертируем в numpy в форму (1,) -> (1,512)\n",
        "token_input = np.asarray([token_input])\n",
        "mask_input = np.asarray([mask_input])\n",
        "seg_input = np.asarray([seg_input])\n",
        "\n",
        "\n",
        "# пропускаем через нейросеть...\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[1]       # в [1] ответ на вопрос, является ли второе предложение логичным по смыслу\n",
        "#print('Sentence is okey: ', not bool(np.argmax(predicts, axis=-1)[0]), predicts)\n",
        "print('Sentence is okey:', int(round(predicts[0][0]*100)), '%')                    # [[0.9657724  0.03422766]] - левое число вероятность что второе предложение подходит по смыслу, а правое - что второе предложение случайное\n",
        "out = int(round(predicts[0][0]*100)) \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Children smiling and waving at camera -> They are smiling at their parents\n",
            "Sentence is okey: 99 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}